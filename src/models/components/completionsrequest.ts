/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod";
import { remap as remap$ } from "../../lib/primitives.js";
import { safeParse } from "../../lib/schemas.js";
import { Result as SafeParseResult } from "../../types/fp.js";
import { SDKValidationError } from "../errors/sdkvalidationerror.js";
import {
  CompletionsPrompt,
  CompletionsPrompt$inboundSchema,
  CompletionsPrompt$Outbound,
  CompletionsPrompt$outboundSchema,
} from "./completionsprompt.js";
import {
  StreamOptions,
  StreamOptions$inboundSchema,
  StreamOptions$Outbound,
  StreamOptions$outboundSchema,
} from "./streamoptions.js";

export type CompletionsRequest = {
  bestOf?: number | null | undefined;
  echo?: boolean | null | undefined;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
   *
   * @remarks
   * existing frequency in the text so far
   */
  frequencyPenalty?: number | null | undefined;
  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * @remarks
   *
   * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
   * to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits
   * generated by the model prior to sampling. The exact effect will vary per model, but values
   * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or
   * 100 should result in a ban or exclusive selection of the relevant token.
   */
  logitBias?: { [k: string]: number } | null | undefined;
  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
   */
  logprobs?: number | null | undefined;
  /**
   * The maximum number of tokens to generate in the chat completion
   */
  maxTokens?: number | null | undefined;
  /**
   * ID of the model to use
   */
  model: string;
  /**
   * How many chat completion choices to generate for each input message
   */
  n?: number | null | undefined;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on
   *
   * @remarks
   * whether they appear in the text so far
   */
  presencePenalty?: number | null | undefined;
  prompt: CompletionsPrompt;
  /**
   * If specified, our system will make a best effort to sample deterministically
   */
  seed?: number | null | undefined;
  /**
   * Up to 4 sequences where the API will stop generating further tokens
   */
  stop?: Array<string> | null | undefined;
  /**
   * Whether to stream back partial progress
   */
  stream?: boolean | null | undefined;
  streamOptions?: StreamOptions | null | undefined;
  /**
   * The suffix that comes after a completion of inserted text.
   */
  suffix?: string | null | undefined;
  /**
   * What sampling temperature to use, between 0 and 2
   */
  temperature?: number | null | undefined;
  /**
   * An alternative to sampling with temperature
   */
  topP?: number | null | undefined;
  /**
   * A unique identifier representing your end-user
   */
  user?: string | null | undefined;
};

/** @internal */
export const CompletionsRequest$inboundSchema: z.ZodType<
  CompletionsRequest,
  z.ZodTypeDef,
  unknown
> = z.object({
  best_of: z.nullable(z.number().int().default(1)),
  echo: z.nullable(z.boolean().default(false)),
  frequency_penalty: z.nullable(z.number()).optional(),
  logit_bias: z.nullable(z.record(z.number())).optional(),
  logprobs: z.nullable(z.number().int()).optional(),
  max_tokens: z.nullable(z.number().int().default(16)),
  model: z.string(),
  n: z.nullable(z.number().int()).optional(),
  presence_penalty: z.nullable(z.number()).optional(),
  prompt: CompletionsPrompt$inboundSchema,
  seed: z.nullable(z.number().int()).optional(),
  stop: z.nullable(z.array(z.string())).optional(),
  stream: z.nullable(z.boolean()).optional(),
  stream_options: z.nullable(StreamOptions$inboundSchema).optional(),
  suffix: z.nullable(z.string()).optional(),
  temperature: z.nullable(z.number()).optional(),
  top_p: z.nullable(z.number()).optional(),
  user: z.nullable(z.string()).optional(),
}).transform((v) => {
  return remap$(v, {
    "best_of": "bestOf",
    "frequency_penalty": "frequencyPenalty",
    "logit_bias": "logitBias",
    "max_tokens": "maxTokens",
    "presence_penalty": "presencePenalty",
    "stream_options": "streamOptions",
    "top_p": "topP",
  });
});

/** @internal */
export type CompletionsRequest$Outbound = {
  best_of: number | null;
  echo: boolean | null;
  frequency_penalty?: number | null | undefined;
  logit_bias?: { [k: string]: number } | null | undefined;
  logprobs?: number | null | undefined;
  max_tokens: number | null;
  model: string;
  n?: number | null | undefined;
  presence_penalty?: number | null | undefined;
  prompt: CompletionsPrompt$Outbound;
  seed?: number | null | undefined;
  stop?: Array<string> | null | undefined;
  stream?: boolean | null | undefined;
  stream_options?: StreamOptions$Outbound | null | undefined;
  suffix?: string | null | undefined;
  temperature?: number | null | undefined;
  top_p?: number | null | undefined;
  user?: string | null | undefined;
};

/** @internal */
export const CompletionsRequest$outboundSchema: z.ZodType<
  CompletionsRequest$Outbound,
  z.ZodTypeDef,
  CompletionsRequest
> = z.object({
  bestOf: z.nullable(z.number().int().default(1)),
  echo: z.nullable(z.boolean().default(false)),
  frequencyPenalty: z.nullable(z.number()).optional(),
  logitBias: z.nullable(z.record(z.number())).optional(),
  logprobs: z.nullable(z.number().int()).optional(),
  maxTokens: z.nullable(z.number().int().default(16)),
  model: z.string(),
  n: z.nullable(z.number().int()).optional(),
  presencePenalty: z.nullable(z.number()).optional(),
  prompt: CompletionsPrompt$outboundSchema,
  seed: z.nullable(z.number().int()).optional(),
  stop: z.nullable(z.array(z.string())).optional(),
  stream: z.nullable(z.boolean()).optional(),
  streamOptions: z.nullable(StreamOptions$outboundSchema).optional(),
  suffix: z.nullable(z.string()).optional(),
  temperature: z.nullable(z.number()).optional(),
  topP: z.nullable(z.number()).optional(),
  user: z.nullable(z.string()).optional(),
}).transform((v) => {
  return remap$(v, {
    bestOf: "best_of",
    frequencyPenalty: "frequency_penalty",
    logitBias: "logit_bias",
    maxTokens: "max_tokens",
    presencePenalty: "presence_penalty",
    streamOptions: "stream_options",
    topP: "top_p",
  });
});

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace CompletionsRequest$ {
  /** @deprecated use `CompletionsRequest$inboundSchema` instead. */
  export const inboundSchema = CompletionsRequest$inboundSchema;
  /** @deprecated use `CompletionsRequest$outboundSchema` instead. */
  export const outboundSchema = CompletionsRequest$outboundSchema;
  /** @deprecated use `CompletionsRequest$Outbound` instead. */
  export type Outbound = CompletionsRequest$Outbound;
}

export function completionsRequestToJSON(
  completionsRequest: CompletionsRequest,
): string {
  return JSON.stringify(
    CompletionsRequest$outboundSchema.parse(completionsRequest),
  );
}

export function completionsRequestFromJSON(
  jsonString: string,
): SafeParseResult<CompletionsRequest, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => CompletionsRequest$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'CompletionsRequest' from JSON`,
  );
}

/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod";
import { remap as remap$ } from "../../lib/primitives.js";
import { safeParse } from "../../lib/schemas.js";
import { Result as SafeParseResult } from "../../types/fp.js";
import { SDKValidationError } from "../errors/sdkvalidationerror.js";
import {
  ChatCompletionMessage,
  ChatCompletionMessage$inboundSchema,
  ChatCompletionMessage$Outbound,
  ChatCompletionMessage$outboundSchema,
} from "./chatcompletionmessage.js";
import {
  ChatCompletionToolsParam,
  ChatCompletionToolsParam$inboundSchema,
  ChatCompletionToolsParam$Outbound,
  ChatCompletionToolsParam$outboundSchema,
} from "./chatcompletiontoolsparam.js";
import {
  ResponseFormat,
  ResponseFormat$inboundSchema,
  ResponseFormat$Outbound,
  ResponseFormat$outboundSchema,
} from "./responseformat.js";
import {
  StreamOptions,
  StreamOptions$inboundSchema,
  StreamOptions$Outbound,
  StreamOptions$outboundSchema,
} from "./streamoptions.js";
import {
  ToolChoice,
  ToolChoice$inboundSchema,
  ToolChoice$Outbound,
  ToolChoice$outboundSchema,
} from "./toolchoice.js";

/**
 * Represents the chat completion request.
 *
 * @remarks
 *
 * This is used to represent the chat completion request in the chat completion request.
 * It can be either a chat completion or a chat completion stream.
 */
export type CreateChatCompletionStreamRequest = {
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
   *
   * @remarks
   * existing frequency in the text so far
   */
  frequencyPenalty?: number | null | undefined;
  /**
   * Controls how the model responds to function calls
   */
  functionCall?: any | undefined;
  /**
   * A list of functions the model may generate JSON inputs for
   */
  functions?: Array<any> | null | undefined;
  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * @remarks
   *
   * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
   * to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits
   * generated by the model prior to sampling. The exact effect will vary per model, but values
   * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or
   * 100 should result in a ban or exclusive selection of the relevant token.
   */
  logitBias?: { [k: string]: number } | null | undefined;
  /**
   * The maximum number of tokens to generate in the chat completion
   */
  maxCompletionTokens?: number | null | undefined;
  /**
   * The maximum number of tokens to generate in the chat completion
   *
   * @deprecated field: This will be removed in a future release, please migrate away from it as soon as possible.
   */
  maxTokens?: number | null | undefined;
  /**
   * A list of messages comprising the conversation so far
   */
  messages: Array<ChatCompletionMessage>;
  /**
   * ID of the model to use
   */
  model: string;
  /**
   * How many chat completion choices to generate for each input message
   */
  n?: number | null | undefined;
  /**
   * Whether to enable parallel tool calls.
   */
  parallelToolCalls?: boolean | null | undefined;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on
   *
   * @remarks
   * whether they appear in the text so far
   */
  presencePenalty?: number | null | undefined;
  responseFormat?: ResponseFormat | null | undefined;
  /**
   * If specified, our system will make a best effort to sample deterministically
   */
  seed?: number | null | undefined;
  /**
   * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
   *
   * @remarks
   *
   * If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
   * If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
   * If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
   * When not set, the default behavior is 'auto'.
   */
  serviceTier?: string | null | undefined;
  /**
   * Up to 4 sequences where the API will stop generating further tokens
   */
  stop?: Array<string> | null | undefined;
  /**
   * Whether to stream back partial progress. Must be true for this request type.
   */
  stream?: boolean | undefined;
  streamOptions?: StreamOptions | null | undefined;
  /**
   * What sampling temperature to use, between 0 and 2
   */
  temperature?: number | null | undefined;
  toolChoice?: ToolChoice | null | undefined;
  /**
   * A list of tools the model may call
   */
  tools?: Array<ChatCompletionToolsParam> | null | undefined;
  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
   *
   * @remarks
   * logprobs must be set to true if this parameter is used.
   */
  topLogprobs?: number | null | undefined;
  /**
   * An alternative to sampling with temperature
   */
  topP?: number | null | undefined;
  /**
   * A unique identifier representing your end-user
   */
  user?: string | null | undefined;
};

/** @internal */
export const CreateChatCompletionStreamRequest$inboundSchema: z.ZodType<
  CreateChatCompletionStreamRequest,
  z.ZodTypeDef,
  unknown
> = z.object({
  frequency_penalty: z.nullable(z.number()).optional(),
  function_call: z.any().optional(),
  functions: z.nullable(z.array(z.any())).optional(),
  logit_bias: z.nullable(z.record(z.number())).optional(),
  max_completion_tokens: z.nullable(z.number().int()).optional(),
  max_tokens: z.nullable(z.number().int()).optional(),
  messages: z.array(ChatCompletionMessage$inboundSchema),
  model: z.string(),
  n: z.nullable(z.number().int()).optional(),
  parallel_tool_calls: z.nullable(z.boolean()).optional(),
  presence_penalty: z.nullable(z.number()).optional(),
  response_format: z.nullable(ResponseFormat$inboundSchema).optional(),
  seed: z.nullable(z.number().int()).optional(),
  service_tier: z.nullable(z.string()).optional(),
  stop: z.nullable(z.array(z.string())).optional(),
  stream: z.boolean().default(true),
  stream_options: z.nullable(StreamOptions$inboundSchema).optional(),
  temperature: z.nullable(z.number()).optional(),
  tool_choice: z.nullable(ToolChoice$inboundSchema).optional(),
  tools: z.nullable(z.array(ChatCompletionToolsParam$inboundSchema)).optional(),
  top_logprobs: z.nullable(z.number().int()).optional(),
  top_p: z.nullable(z.number()).optional(),
  user: z.nullable(z.string()).optional(),
}).transform((v) => {
  return remap$(v, {
    "frequency_penalty": "frequencyPenalty",
    "function_call": "functionCall",
    "logit_bias": "logitBias",
    "max_completion_tokens": "maxCompletionTokens",
    "max_tokens": "maxTokens",
    "parallel_tool_calls": "parallelToolCalls",
    "presence_penalty": "presencePenalty",
    "response_format": "responseFormat",
    "service_tier": "serviceTier",
    "stream_options": "streamOptions",
    "tool_choice": "toolChoice",
    "top_logprobs": "topLogprobs",
    "top_p": "topP",
  });
});

/** @internal */
export type CreateChatCompletionStreamRequest$Outbound = {
  frequency_penalty?: number | null | undefined;
  function_call?: any | undefined;
  functions?: Array<any> | null | undefined;
  logit_bias?: { [k: string]: number } | null | undefined;
  max_completion_tokens?: number | null | undefined;
  max_tokens?: number | null | undefined;
  messages: Array<ChatCompletionMessage$Outbound>;
  model: string;
  n?: number | null | undefined;
  parallel_tool_calls?: boolean | null | undefined;
  presence_penalty?: number | null | undefined;
  response_format?: ResponseFormat$Outbound | null | undefined;
  seed?: number | null | undefined;
  service_tier?: string | null | undefined;
  stop?: Array<string> | null | undefined;
  stream: boolean;
  stream_options?: StreamOptions$Outbound | null | undefined;
  temperature?: number | null | undefined;
  tool_choice?: ToolChoice$Outbound | null | undefined;
  tools?: Array<ChatCompletionToolsParam$Outbound> | null | undefined;
  top_logprobs?: number | null | undefined;
  top_p?: number | null | undefined;
  user?: string | null | undefined;
};

/** @internal */
export const CreateChatCompletionStreamRequest$outboundSchema: z.ZodType<
  CreateChatCompletionStreamRequest$Outbound,
  z.ZodTypeDef,
  CreateChatCompletionStreamRequest
> = z.object({
  frequencyPenalty: z.nullable(z.number()).optional(),
  functionCall: z.any().optional(),
  functions: z.nullable(z.array(z.any())).optional(),
  logitBias: z.nullable(z.record(z.number())).optional(),
  maxCompletionTokens: z.nullable(z.number().int()).optional(),
  maxTokens: z.nullable(z.number().int()).optional(),
  messages: z.array(ChatCompletionMessage$outboundSchema),
  model: z.string(),
  n: z.nullable(z.number().int()).optional(),
  parallelToolCalls: z.nullable(z.boolean()).optional(),
  presencePenalty: z.nullable(z.number()).optional(),
  responseFormat: z.nullable(ResponseFormat$outboundSchema).optional(),
  seed: z.nullable(z.number().int()).optional(),
  serviceTier: z.nullable(z.string()).optional(),
  stop: z.nullable(z.array(z.string())).optional(),
  stream: z.boolean().default(true),
  streamOptions: z.nullable(StreamOptions$outboundSchema).optional(),
  temperature: z.nullable(z.number()).optional(),
  toolChoice: z.nullable(ToolChoice$outboundSchema).optional(),
  tools: z.nullable(z.array(ChatCompletionToolsParam$outboundSchema))
    .optional(),
  topLogprobs: z.nullable(z.number().int()).optional(),
  topP: z.nullable(z.number()).optional(),
  user: z.nullable(z.string()).optional(),
}).transform((v) => {
  return remap$(v, {
    frequencyPenalty: "frequency_penalty",
    functionCall: "function_call",
    logitBias: "logit_bias",
    maxCompletionTokens: "max_completion_tokens",
    maxTokens: "max_tokens",
    parallelToolCalls: "parallel_tool_calls",
    presencePenalty: "presence_penalty",
    responseFormat: "response_format",
    serviceTier: "service_tier",
    streamOptions: "stream_options",
    toolChoice: "tool_choice",
    topLogprobs: "top_logprobs",
    topP: "top_p",
  });
});

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace CreateChatCompletionStreamRequest$ {
  /** @deprecated use `CreateChatCompletionStreamRequest$inboundSchema` instead. */
  export const inboundSchema = CreateChatCompletionStreamRequest$inboundSchema;
  /** @deprecated use `CreateChatCompletionStreamRequest$outboundSchema` instead. */
  export const outboundSchema =
    CreateChatCompletionStreamRequest$outboundSchema;
  /** @deprecated use `CreateChatCompletionStreamRequest$Outbound` instead. */
  export type Outbound = CreateChatCompletionStreamRequest$Outbound;
}

export function createChatCompletionStreamRequestToJSON(
  createChatCompletionStreamRequest: CreateChatCompletionStreamRequest,
): string {
  return JSON.stringify(
    CreateChatCompletionStreamRequest$outboundSchema.parse(
      createChatCompletionStreamRequest,
    ),
  );
}

export function createChatCompletionStreamRequestFromJSON(
  jsonString: string,
): SafeParseResult<CreateChatCompletionStreamRequest, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => CreateChatCompletionStreamRequest$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'CreateChatCompletionStreamRequest' from JSON`,
  );
}
